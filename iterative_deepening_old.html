<html>
  <!--=======================================================================-->

  <head>
    <title>chicken_littler</title>
    <script
      type="text/javascript"
      src="http://epilog.stanford.edu/javascript/epilog.js"
    ></script>
    <script
      type="text/javascript"
      src="http://gamemaster.stanford.edu/javascript/localstorage.js"
    ></script>
    <script
      type="text/javascript"
      src="http://gamemaster.stanford.edu/interpreter/general.js"
    ></script>
    <script type="text/javascript">
      //==============================================================================
      // This player uses iterative deepening minimax with alpha-beta pruning to select moves.
      // It assumes the opponent plays optimally to minimize our score.
      // 
      // It uses a hybrid evaluation function depending on the structure of the game:
      // - If the game has intermediate rewards and a manageable branching factor, we use a weighted 
      //   heuristic combining mobility, reward proximity, and pessimism about terminal outcomes.
      // - If the game has sparse rewards or high branching factor, we switch to Monte Carlo simulation
      //   to estimate state value more robustly.
      //
      // We use Bigswitch to decide between these two evaluation modes based on a few short rollouts 
      // at the beginning.
      //
      // During the start clock, we perform a headstart minimax search to select an opening move.
      //
      // During gameplay, we balance exploration and exploitation with an epsilon-greedy strategy 
      // that favors random moves early (for exploration) and decays epsilon over time (toward exploitation).
      //==============================================================================

      var manager = "manager";
      var player = "chicken_littler";

      var role = "robot";
      var rules = [];
      var startclock = 10;
      var playclock = 10;

      var library = [];
      var roles = [];
      var state = [];
      var epsilon = 0.2; // Initial exploration rate of 20%
      var headstartMove = null; // store first move found during headstart
      var useMonteCarlo = false; // Bigswitch toggles this

      //==============================================================================

      function shuffle(array) {
        for (var i = array.length - 1; i > 0; i--) {
          var j = Math.floor(Math.random() * (i + 1));
          var temp = array[i];
          array[i] = array[j];
          array[j] = temp;
        }
        return array;
      }

      function ping() {
        return "ready";
      }

      function start(r, rs, sc, pc) {
        role = r;
        rules = rs.slice(1);
        startclock = numberize(sc);
        playclock = numberize(pc);
        library = definemorerules([], rules);
        roles = findroles(library);
        state = findinits(library);
        // time buffer
        var deadline = Date.now() + (startclock - 3.0) * 1000;
        // fallback move
        headstartMove = findlegals(state, library)[0];
        var best = headstartMove;
        // use iterative deepening to explore the game tree during the start clock
        for (var depth = 1; depth <= 8; depth++) {
          if (Date.now() > deadline) break;
          var action = playminimaxidinner(state, depth, deadline);
          if (action === false) break;
          best = action;
        }
        headstartMove = best;

        // this function runs four simulations  to preliminarily identify the reward structure
        // of the game by looking at branching factor, whether there exists an intermediate
        // reward, and whether there is a good evaluation signal
        function bigswitchInit() {
          var simulations = 4;
          var totalReward = 0;
          var totalRewardStates = 0;
          var totalMoves = 0;
          var totalSteps = 0;

          for (var i = 0; i < simulations; i++) {
            var s = state;
            for (var j = 0; j < 5; j++) {
              if (findterminalp(s, library)) break;

              var actions = findlegals(s, library);
              if (actions.length === 0) break;

              totalMoves += actions.length;
              totalSteps++;

              s = simulate(
                actions[Math.floor(Math.random() * actions.length)],
                s,
                library
              );

              var reward = findreward(role, s, library);
              totalReward += reward;
              if (reward > 0) totalRewardStates++;
            }
          }
          var avgBranching = totalMoves / totalSteps;
          var avgReward = totalReward / (simulations * 5);
          var rewardDensity = totalRewardStates / (simulations * 5);
          // Use Monte Carlo evaluation function if rewards are sparse or
          // branching is very high because an eval heuristic is less reliable
          // in these instances
          if (avgBranching > 25 || (avgReward < 5 && rewardDensity < 0.1)) {
            useMonteCarlo = true;
          } else {
            useMonteCarlo = false;
          }
          console.log("Bigswitch decided:", {
            avgBranching,
            avgReward,
            rewardDensity,
            useMonteCarlo,
          });
        }
        bigswitchInit();
        return "ready";
      }

      // Pessimistic Evaluation Function
      // Returns actual reward only for terminal states, otherwise 0
      function pessimisticEval(state) {
        if (findterminalp(state, library)) {
          return findreward(role, state, library) * 1;
        }
        return 0;
      }

      // Mobility Function
      // Returns a high score when player in control has many moves (if us), or when they have few moves (if opponent)
      function mobilityEval(state) {
        var controller = findcontrol(state, library);
        var actions = findlegals(state, library); // Legal moves for controller in this state
        var feasibles = findactions(library); // All possible actions in the game
        var maxActions = Math.max(feasibles.length, 1); // Prevent division by zero

        if (controller === role) {
          // More moves for us is better
          return Math.floor((actions.length / maxActions) * 100);
        } else {
          // Fewer moves for the opponent is better for us
          return Math.floor(((maxActions - actions.length) / maxActions) * 100);
        }
      }

      // Intermediate Reward Function
      // Returns the actual reward for the current state, whether or not it is terminal
      function intermediateRewardEval(state) {
        return findreward(role, state, library) * 1;
      }

      // This function uses a combination of evaluation heuristics so that it is well rounded and can adapt
      // well to a variety of game needs. Currently, our evaluation breakdown is:
      // 0.3 * mobility
      // 0.3 * pessimistic
      // 0.4 * intermediateReward
      function combinedEval(state) {
        // If this is a terminal state and we win, return maximum score
        if (
          findterminalp(state, library) &&
          findreward(role, state, library) === 100
        ) {
          return 100;
        }
        // If this is a terminal state and we lose, return minimum score
        if (
          findterminalp(state, library) &&
          findreward(role, state, library) === 0
        ) {
          return 0;
        }
        return (
          0.3 * mobilityEval(state) +
          0.4 * intermediateRewardEval(state) +
          0.3 * pessimisticEval(state)
        );
      }

      // simulate rewards from potential move sequences and take the average
      // over all simulations to get expected reward from a current state.
      // use discounted reward to place higher weight on faster wins.
      function monteCarloEval(state, deadline, rollouts = 5) {
        var total = 0;
        var maxDepth = 10;
        var gamma = 0.9; // Discount factor for reward weighting

        for (var i = 0; i < rollouts; i++) {
          var rolloutState = state;
          var depth = 0;

          while (!findterminalp(rolloutState, library) && depth < maxDepth) {
            if (Date.now() > deadline - 100) break;

            var actions = findlegals(rolloutState, library);
            if (actions.length === 0) break;

            var action = actions[Math.floor(Math.random() * actions.length)];
            rolloutState = simulate(action, rolloutState, library);
            depth++;
          }

          var reward = findreward(role, rolloutState, library);
          var discountedReward = reward * Math.pow(gamma, depth);
          total += discountedReward;
        }
        return total / rollouts;
      }


      function play(move) {
        if (typeof move !== "undefined" && move !== null) {
          state = simulate(move, state, library);
        }
        if (findcontrol(state, library) !== role) return false;

        var legals = findlegals(state, library);
        if (legals.length === 0) return false;
        // check if first move and if so, use whatever found from headstart
        if (headstartMove !== null) {
          var moveToPlay = headstartMove;
          headstartMove = null;
          return moveToPlay;
        }
        var best = playminimaxid();
        if (best === false || typeof best === "undefined") {
          console.warn(
            "Fallback: timeout or no move found, choosing random move"
          );
          return legals[0]; // fallback
        }
        return best;
      }

      // this function uses iterative deepening to search the tree iteratively at each depth
      function playminimaxid() {
        var deadline = Date.now() + (playclock - 3.0) * 1000;
        var best = findlegals(state, library)[0];

        // iterates through tree levels 1 through 4
        for (var depth = 1; depth <= 3; depth++) {
          if (Date.now() > deadline) {
            break;
          }
          // find best move at each depth
          var action = playminimaxidinner(state, depth, deadline);
          if (action === false) {
            return best;
          }
          best = action;
          if (Date.now() > deadline) {
            break;
          }
        }

        // Îµ-greedy: explore randomly with probability epsilon
        // epsilon decreases after each move to prioritize exploration less over time
        // and exploitation more as become more confident in moves
        if (headstartMove === null) {
          // If we already used the headstart move, decay slowly
          epsilon = Math.max(0.01, epsilon * 0.95);
        } else {
          // If still using early-game logic, decay faster since want to exploit more on path
          epsilon = Math.max(0.01, epsilon * 0.8);
        }
        if (Math.random() < epsilon) {
          var legals = findlegals(state, library);
          if (legals.length > 0) {
            return legals[Math.floor(Math.random() * legals.length)];
          }
        }
        // Otherwise exploit
        return best;
      }

      //this function simulates all possible moves from a state and calls
      //minimaxid to determine which is the best move
      function playminimaxidinner(state, depth, deadline) {
        var actions = shuffle(findlegals(state, library));
        var best = actions[0];
        var score = 0;
        for (var i = 0; i < actions.length; i++) {
          // check to make sure haven't reached deadline
          if (Date.now() > deadline) return false;
          var newstate = simulate(actions[i], state, library);
          var newscore = minimaxid(newstate, depth, deadline, 0, 100);
          if (newscore === false) {
            return false;
          }
          // if find action that gets perfect score, return it immediately
          if (newscore === 100) {
            return actions[i];
          }
          // keep track of best score so far
          if (newscore > score) {
            best = actions[i];
            score = newscore;
          }
        }
        return best;
      }

      function minimaxid(state, depth, deadline, alpha, beta) {
        if (findterminalp(state, library)) {
          return findreward(role, state, library) * 1;
        }
        if (depth <= 0) {
          console.log("depth <= 0, eval function reached");
          return useMonteCarlo
            ? monteCarloEval(state, deadline)
            : combinedEval(state);
        }
        if (Date.now() > deadline) {
          return combinedEval(state);
        }
        if (findcontrol(state, library) === role) {
          return maxscore(state, depth, deadline, 0, 100); // alpha = 0, beta = 100
        }
        return minscore(state, depth, deadline, 0, 100);
      }

      function maxscore(state, depth, deadline, alpha, beta) {
        var actions = findlegals(state, library);
        if (actions.length === 0) return 0;
        var score = 0;
        for (var i = 0; i < actions.length; i++) {
          if (Date.now() > deadline) return combinedEval(state);
          var newstate = simulate(actions[i], state, library);
          var newscore = minimaxid(newstate, depth - 1, deadline, alpha, beta);
          if (newscore === false) return false;
          if (newscore === 100) return 100; // winning shortcut

          if (newscore > score) score = newscore;
          if (score > alpha) alpha = score;
          if (alpha >= beta) break; // pruning happens here
        }
        return score;
      }

      function minscore(state, depth, deadline, alpha, beta) {
        var actions = findlegals(state, library);
        if (actions.length === 0) return 0;
        var score = 100;
        for (var i = 0; i < actions.length; i++) {
          if (Date.now() > deadline) return combinedEval(state);
          var newstate = simulate(actions[i], state, library);
          var newscore = minimaxid(newstate, depth - 1, deadline, alpha, beta);
          if (newscore === false) return false;
          if (newscore === 0) return 0; // loss shortcut

          if (newscore < score) score = newscore;
          if (score < beta) beta = score;
          if (alpha >= beta) break; // pruning happens here
        }
        return score;
      }

      function stop(move) {
        return false;
      }

      function abort() {
        return false;
      }

      //==============================================================================
      // End of player code
      //==============================================================================
    </script>
  </head>

  <!--=======================================================================-->

  <body bgcolor="#aabbbb" onload="doinitialize()">
    <center>
      <table width="720" cellspacing="0" cellpadding="40" bgcolor="#ffffff">
        <tr>
          <td>
            <!--=======================================================================-->

            <center>
              <table width="640" cellpadding="0">
                <tr>
                  <td width="180" align="center" valign="center">
                    <img
                      width="130"
                      src="http://gamemaster.stanford.edu/images/ggp.jpg"
                    />
                  </td>
                  <td align="center">
                    <span style="font-size: 18pt">&nbsp;</span>
                    <span style="font-size: 32pt">Gamemaster</span><br />
                  </td>
                  <td
                    width="180"
                    align="center"
                    style="color: #000066; font-size: 18px"
                  >
                    <i>General<br />Game<br />Playing</i>
                  </td>
                </tr>
              </table>
            </center>

            <!--=======================================================================-->

            <br />
            <table
              width="640"
              cellpadding="8"
              cellspacing="0"
              bgcolor="#f4f8f8"
              border="1"
            >
              <tr height="40">
                <td align="center">
                  <table style="color: #000066; font-size: 18px">
                    <tr>
                      <td>
                        Protocol: localstorage<br />
                        Metagamer: none<br />
                        Strategy: custom<br />
                        Identifier: <span id="player">chicken_littler</span>
                        <img
                          src="http://gamemaster.stanford.edu/images/pencil.gif"
                          onclick="doplayer()"
                        />
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            </table>
            <br />

            <!--=======================================================================-->

            <center>
              <br />
              <textarea
                id="transcript"
                style="font-family: courier"
                rows="30"
                cols="80"
                readonly
              ></textarea>
            </center>

            <!--=======================================================================-->
          </td>
        </tr>
      </table>
    </center>
  </body>

  <!--=======================================================================-->
</html>
